---
title: "modeling"
author: "Gabriel Krotkov"
date: "2024-04-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE}
library(tidyverse)
library(randomForest)
library(kernlab)
library(MASS)
library(e1071)
library(gbm)
library(caret)
library(mclust)
library(neuralnet)
```

```{r data loading, message = FALSE}
raw <- read_csv("../data/train_data.csv")
data <- raw
# center and scale all variables
# remove scaling because it's acting weird with the validation
# data <- apply(raw[, which(colnames(raw) != "Y")], 2, log)
# data <- data.frame(cbind(data, Y = raw$Y))

# cast to a factor
data$Y <- factor(data$Y)

# 80/20 train/test split
parts = createDataPartition(data$Y, p = 0.8, list = FALSE)
train = data[parts, ]
test = data[-parts, ]

validation <- read_csv("../data/test_data_x.csv")
```

```{r fitting after hyperparam tuning}
team.name <- "Jing's Favorite Students"
# mtry = 5, ntree = 1000
rf <- randomForest(Y ~ ., ntree = 5000, mtry = 5)
preds <- predict(rf, newdata = test)
test.acc <- 1 - (sum(preds == test$Y) / nrow(test))
rf <- randomForest(Y ~ ., data = data, ntree = 5000)
y.guesses <- predict(rf, newdata = validation)

save(list=c("y.guesses", "test.acc", "team.name"),
     file="../data/jings_favorite_students.RData")
```

```{r GMMs}
fit <- MclustDA(train[, setdiff(colnames(train), "Y")], train$Y)
```

```{r train-PCA}
pca <- prcomp(train[, which(colnames(train) != "Y")], 
              center = TRUE, scale = TRUE)

viz <- data.frame(pca$x)
viz$label <- as.numeric(train$Y) - 1
viz$label[err_idx] <- ifelse(viz$label[err_idx] == 0, -1, -2)
viz$label <- factor(viz$label, labels = c("False Negative", "False Positive", 
                                        "True Negative", "True Positive"))
ggplot(viz, aes(x = PC1, y = PC2, col = label)) + 
    geom_point() + 
    labs(title = "PCA With Error Analysis", 
         col = "Label")
```


```{r RF only on PCs}
# nope, does not work
pca <- prcomp(train[, which(colnames(train) != "Y")], 
              center = TRUE, scale = TRUE)
train <- data.frame(pca$x, Y = train$Y)

rf <- randomForest(Y ~ ., data = train, ntree = 1000)

pca <- prcomp(test[, which(colnames(test) != "Y")], 
              center = TRUE, scale = TRUE)
test <- data.frame(pca$x, Y = test$Y)

preds <- predict(rf, newdata = test)
test.acc <- 1 - (sum(preds == test$Y) / nrow(test))
```

```{r random forest prove that dropping area improves the model}
reps <- 10
cv_errors <- rep(0, reps)
cv_errors_dropped <- rep(0, reps)

for (i in 1:reps){
    rf <- randomForest(Y ~ ., data = train)
    rf_dropped <- randomForest(Y ~ . - Area, data = train)
    preds <- predict(rf, newdata = test)
    preds_dropped <- predict(rf_dropped, newdata = test)
    cv_errors[i] <- 1 - (sum(test$Y == preds) / nrow(test))
    cv_errors_dropped[i] <- 1 - (sum(test$Y == preds_dropped) / nrow(test))
}

cat("Mean difference (positive means dropping helped):",
    mean(cv_errors - cv_errors_dropped), "\n")
```

```{r testing dropping PC1, include = FALSE}
# include first principal component for both train and test
pca <- prcomp(train[, which(colnames(train) != "Y")], 
              center = TRUE, scale = TRUE)
train$pc1 <- pca$x[, 1]

pca <- prcomp(test[, which(colnames(test) != "Y")], 
              center = TRUE, scale = TRUE)

test$pc1 <- pca$x[, 1]
reps <- 100
cv_errors <- rep(0, reps)
cv_errors_dropped <- rep(0, reps)

for (i in 1:reps){
    rf <- randomForest(Y ~ ., data = train)
    rf_dropped <- randomForest(Y ~ . - pc1, data = train)
    preds <- predict(rf, newdata = test)
    preds_dropped <- predict(rf_dropped, newdata = test)
    cv_errors[i] <- 1 - (sum(test$Y == preds) / nrow(test))
    cv_errors_dropped[i] <- 1 - (sum(test$Y == preds_dropped) / nrow(test))
}

cat("Mean difference (positive means dropping helped):",
    mean(cv_errors - cv_errors_dropped), "\n")
```

Looks like neither including PC scores nor dropping Area consistently improves performance.

```{r hyperparameter tuning}
ntrees <- seq(100, 1000, by = 50)
mtrys <- 5:11
reps <- 10

ntree_errors <- rep(0, length(ntrees))
names(ntree_errors) <- ntrees

mtry_errors <- rep(0, length(mtrys))
names(mtry_errors) <- mtrys

for (i in 1:length(ntrees)){
    tmp <- rep(0, reps)
    for (j in 1:reps){
        rf <- randomForest(Y ~ ., data = train, ntree = ntrees[i])
        preds <- predict(rf, newdata = test)
        tmp[j] <- 1 - (sum(test$Y == preds) / nrow(test))
    }
    ntree_errors[i] <- mean(tmp)
}

for (i in 1:length(mtrys)){
    tmp <- rep(0, reps)
    for (j in 1:reps){
        rf <- randomForest(Y ~ ., data = train, mtry = mtrys[i], 
                           ntree = 1000)
        preds <- predict(rf, newdata = test)
        tmp[j] <- 1 - (sum(test$Y == preds) / nrow(test))
    }
    mtry_errors[i] <- mean(tmp)
}

# 5 appears to be the best selection for
which.min(mtry_errors)
```

```{r}
reps <- 10
cv_error <- rep(0, reps)
for (i in 1:reps){
    rf <- randomForest(Y ~ ., data = train)
    preds <- predict(rf, newdata = test)
    cv_error[i] <- 1 - (sum(test$Y == preds) / nrow(test))
}
```

```{r boosting}
# Train the GBM model
gbm_model <- gbm(Y ~ ., 
                 data = train, distribution = "adaboost", 
                 shrinkage = 0.001, n.minobsinnode = 10, n.trees = 10000)
probs <- predict(gbm_model, newdata = test, 
                 n.trees = 100, 
                 type = "response")
cutoff <- quantile(probs, table(train$Y)[1] / nrow(train))
preds <- ifelse(probs >= cutoff, 1, 0)
cv_error <- 1 - (sum(test$Y == preds) / nrow(test))

cv_error
```

```{r}
draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Class1', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Class2', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Class1', cex=1.2, srt=90)
  text(140, 335, 'Class2', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  
```

