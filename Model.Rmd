---
title: "Feature Selection"
output: pdf_document
---
# Feature Engineering and Selection:
Based on your EDA, decide which variables to keep, drop, or transform.
Create new features that might be useful for your model, based on your understanding of the dataset and the relationships among variables.
Perform any necessary transformations (e.g., scaling, normalization) that might help your modeling process.
NOTE: feature engineering better for predictions.

We probably do NOT need to do feature selection OR engineering...

## NOTES
- Only use area, not perimeter, axis length, etc.

# Load in data
```{r}
# Load in the data
train <- read.csv("data/train_data.csv")
```

```{r}
# Check for multicollinearity
# library(car)
# vif_values <- vif(full_model)
# rounded_vif <- round(vif_values, 4)
# rounded_vif
```

Area, Perimeter, MajorAxisLength, MinorAxisLength, AspectRation, Eccentricity, ConvexArea, EquivDiameter, Compactness, ShapeFactor1, ShapeFactor2, and ShapeFactor3 all have very high VIF values, indicating extremely high multicollinearity.

Extent and ShapeFactor4 have VIF values around 1 and 220 respectively. It seems that Extent has little to no collinearity with other variables, while ShapeFactor4 is moderately correlated with others.

# Conclusions based on EDA
### 1. Variables to Consider Including
   - **ShapeFactor1, ShapeFactor2, ShapeFactor3, ShapeFactor4**: These variables showed significant non-zero coefficients in the LASSO regression, so they're likely to be important predictors.
   - **Extent**: This variable demonstrated a low Variance Inflation Factor (VIF)
    - This suggests minimal collinearity with other variables.
   - **EquivDiameter**: This variable has a high VIF and showed a significant effect in the regression models. If multicollinearity concerns are addressed, it might be useful to include (?)
   - **Solidity, roundness, Compactness**: These showed significant coefficients in the predictive models.

### 2. Variables to Potentially Exclude
   - **Area, Perimeter, MajorAxisLength, MinorAxisLength**: These variables exhibit extremely high multicollinearity (very high VIF values) and were not consistently significant across the models.
    - Indications: Including one of these variables might be sufficient if needed, since they are highly correlated with each other.
   - **AspectRation, Eccentricity**: High correlation with each other and other shape descriptors, potentially leading to issues of multicollinearity.

# LAST STEP:
AFTER selecting which variables we want to include

# Splitting data into training and validation (NO FEATURE SELECTION)

```{r}
# Splitting the data into training and validation sets
set.seed(1234)
index <- sample(1:nrow(train), round(0.8 * nrow(train)))
train_set <- train[index, ]
validation_set <- train[-index, ]
```

# LASSO
Proceeding without standardizing:
```{r, warning=FALSE, message=FALSE}
# Use Lasso?
library(glmnet)
x_train <- model.matrix(Y ~ . - 1, data = train_set) # -1 to exclude intercept
y_train <- train_set$Y
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)
plot(lasso_model)

# Predictions for the validation set
x_validation <- model.matrix(Y ~ . - 1, data = validation_set)
predictions <- predict(lasso_model, s = "lambda.min", newx = x_validation)
```

```{r}
# Extract coefficients of the model at lambda.min
lasso_coefficients <- coef(lasso_model, s = "lambda.min")

# Non-zero coefficients
print(round(lasso_coefficients[lasso_coefficients != 0], 6))
```

## Coefficients with variable names
```{r}
lasso_coefficients <- coef(lasso_model, s = "lambda.min")
variable_names <- colnames(x_train)
coef_w_names <- setNames(lasso_coefficients[-1], variable_names)
print(round(coef_w_names[coef_w_names != 0], 6))
```

Less significant variables:
- Area
- Perimeter
- Major Axis Length
- Convex Area
- EquivDiameter (?)

More significant variables:
- Shape factor 1
- Shape factor 2
- Shape factor 3
- Shape factor 4

Everything else TBD

```{r}
# Predict using the validation set
predictions <- predict(lasso_model, s = "lambda.min", newx = x_validation)

# MSE on the validation set
mse <- mean((validation_set$Y - predictions)^2)
mse
```
```{r}
# Accuracy
classes <- ifelse(predictions > 0.5, 1, 0)

real.classes <- validation_set$Y
correct.predictions <- sum(classes == real.classes)
total.predictions <- length(classes)
accuracy.percentage <- (correct.predictions / total.predictions) * 100
accuracy.percentage
```


# Ridge
```{r}
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0)

# CV curve
plot(ridge_model)
```

```{r}
# Predictions for the validation set
ridge_predictions <- predict(ridge_model, s = "lambda.min", newx = x_validation)

# Extract coefficients of the Ridge model at lambda.min
ridge_coefficients <- coef(ridge_model, s = "lambda.min")

# Non-zero coefficients
print(round(ridge_coefficients, 6))
```

```{r}
# MSE for the validation set 
ridge_mse <- mean((validation_set$Y - ridge_predictions)^2)
ridge_mse
```

```{r}
predicted_classes_ridge <- ifelse(ridge_predictions > 0.5, 1, 0)

actual_classes_ridge <- validation_set$Y
correct_predictions_ridge <- sum(predicted_classes_ridge == actual_classes_ridge)
total_predictions_ridge <- length(predicted_classes_ridge)
accuracy_percentage_ridge <- (correct_predictions_ridge / total_predictions_ridge) * 100
accuracy_percentage_ridge
```

# Logistic
```{r}
library(glmnet)

train_set$Y <- as.factor(train_set$Y)

# Create a matrix of predictors
x_train <- model.matrix(Y ~ . - 1, data = train_set)
y_train <- train_set$Y

# Fit the logistic regression model with cross-validation
cv_logistic_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0)

# Cross-validation plot
plot(cv_logistic_model)
```

```{r}
# Predict on the validation set
x_validation <- model.matrix(Y ~ . - 1, data = validation_set) # -1 to exclude intercept
logistic_predictions <- predict(cv_logistic_model, newx = x_validation, type = "response", s = "lambda.min")
```

```{r}
# MSE for logistic 
logistic_mse <- mean((validation_set$Y - logistic_predictions)^2)
logistic_mse
```
```{r}
# Accuracy
bin <- ifelse(logistic_predictions > 0.5, 1, 0)
bin <- as.factor(bin) 

actual <- as.factor(validation_set$Y)

accuracy_pct <- sum(bin == actual) / length(actual)
accuracy_pct
```
Okay so a plan is to NOT do ANY feature selection at all.

# LOGISTIC AFTER VARIABLE SELECTION
## Make a new Data Frame with variables to include
```{r}
# Variables we want to keep
keep <- c("ShapeFactor1", "ShapeFactor2", "ShapeFactor3", "ShapeFactor4", 
                       "Extent", "Area", "Solidity", "roundness", "Compactness")

train_filtered <- train[, keep]
add_Y <- c(keep, "Y")
# Add response variable
train_final <- train[, add_Y]

# Subset the validation set to include only these variables
validation_set_filtered <- validation_set[, keep]

# Create the model matrix for the filtered validation set
x_validation_final <- model.matrix(~ . - 1, data = validation_set_filtered)
```

```{r}
library(glmnet)

train_final$Y <- as.factor(train_final$Y)

# Create a matrix of predictors
x_train_final <- model.matrix(Y ~ . - 1, data = train_final) # -1 to exclude intercept
y_train_final <- train_final$Y

# Fit the logistic regression model with cross-validation
cv_logistic_model_final <- cv.glmnet(x_train_final, y_train_final, family = "binomial", alpha = 0)

# Cross-validation plot
plot(cv_logistic_model_final)
```

```{r}
# Predict on the validation set
logistic_predictions_final <- predict(cv_logistic_model_final, newx = x_validation_final, type = "response", s = "lambda.min")

# MSE for logistic 
logistic_mse_final <- mean((validation_set$Y - logistic_predictions_final)^2)
logistic_mse_final
```

```{r}
# Calculate accuracy
binary_predictions_final <- ifelse(logistic_predictions_final > 0.5, 1, 0)
binary_predictions_final <- as.factor(binary_predictions_final) 

actual_responses <- as.factor(validation_set$Y)

accuracy <- sum(binary_predictions_final == actual_responses) / length(actual_responses)
accuracy
```

Variable selection made the MSE worse?

# LOGISTIC REDO
```{r}
train <- read.csv("data/train_data.csv")
train$Y <- factor(train$Y, levels = c(0, 1))

# Splitting the data into training and validation sets
set.seed(1234)
index <- sample(1:nrow(train), round(0.8 * nrow(train)))
train_set <- train[index, ]
validation <- train[-index, ]

logistic_model <- glm(Y ~ ., family = "binomial", data = train_set)
predictions <- predict(logistic_model, validation, type = "response")
```
# REDO 2
```{r}
library(glmnet)

train <- read.csv("data/train_data.csv")
train$Y <- factor(train$Y, levels = c(0, 1))

# Splitting the data into training and validation sets
set.seed(1234)
index <- sample(1:nrow(train), round(0.8 * nrow(train)))
train_set <- train[index, ]
validation <- train[-index, ]


# Create model matrices for logistic regression with Lasso
x_train <- model.matrix(Y ~ . - 1, data = train_set)  # Exclude the intercept
y_train <- train_set$Y

# Fit Lasso logistic regression (alpha = 1 for Lasso)
cv_lasso_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)  # Cross-validated Lasso

# Plot the cross-validation results to see how lambda affects the model
plot(cv_lasso_model)
```
```{r}
# Choose the optimal lambda (use either lambda.min or lambda.1se)
optimal_lambda <- cv_lasso_model$lambda.min  # or cv_lasso_model$lambda.1se

# Fit the final logistic regression model with the optimal lambda
final_lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = optimal_lambda)

# Create a model matrix for the validation set
x_validation <- model.matrix(Y ~ . - 1, data = validation)
y_validation <- validation$Y

# Predict probabilities on the validation set
predicted_probabilities <- predict(final_lasso_model, newx = x_validation, type = "response")

# Convert probabilities to binary class predictions (threshold = 0.5)
predicted_classes <- ifelse(predicted_probabilities >= 0.5, 1, 0)

# Calculate accuracy
correct_predictions <- sum(predicted_classes == as.numeric(as.character(y_validation)))
total_predictions <- length(y_validation)
accuracy <- (correct_predictions / total_predictions) * 100

cat("Accuracy on validation set:", accuracy, "%\n")
```


## failed
```{r}
# Get coefficients at the optimal lambda
lasso_coefficients <- coef(cv_lasso_model, s = "lambda.min")
print(lasso_coefficients)  # Coefficients at optimal lambda
```

```{r}
# Extract the coefficients at the optimal lambda
lasso_coefficients <- coef(cv_lasso_model, s = "lambda.min")

# Convert to a matrix to facilitate extraction
coef_matrix <- as.matrix(lasso_coefficients)

# Extract the row names where coefficients are non-zero
important_features <- rownames(coef_matrix)[coef_matrix != 0]

# Display the non-zero coefficients and their associated row names
print(important_features)
```

```{r}
# Create formula using the important features selected by Lasso
formula <- as.formula(paste("Y ~", paste(important_features[-1], collapse = " + ")))

# Fit a logistic regression model using the selected features
logistic_model <- glm(formula, family = "binomial", data = train_set)

# Predict on the validation set
x_validation <- model.matrix(formula, data = validation)[, -1]
predictions <- predict(logistic_model, newx = x_validation, type = "response")
```

```{r}
# Convert probabilities to binary predictions using a threshold
binary_predictions <- ifelse(predictions >= 0.5, 1, 0)

# Calculate accuracy
accuracy <- mean(binary_predictions == validation$Y)
print(paste("Accuracy:", accuracy))
```

```{r}
# Use confusion matrix for detailed evaluation
library(caret)
confusion_matrix <- confusionMatrix(as.factor(binary_predictions), validation$Y)
print(confusion_matrix)
```
# let's try SVM
```{r}
# Required libraries
library(e1071)  # For SVM
library(caret)  # For data splitting and model tuning

# Load your dataset
train <- read.csv("data/train_data.csv")

# Splitting the data into training and validation sets
set.seed(1234)  # For reproducibility
index <- sample(1:nrow(train), round(0.8 * nrow(train)))
train_set <- train[index, ]
validation_set <- train[-index, ]

# Feature scaling
# SVM models benefit from feature scaling to ensure each feature contributes equally to the prediction
scaler <- preProcess(train_set[, -ncol(train_set)], method = c("center", "scale"))
train_scaled <- predict(scaler, train_set)
validation_scaled <- predict(scaler, validation_set)

# SVM with radial basis function (RBF) kernel
# You can also try other kernels like linear, polynomial, or sigmoid
svm_model <- svm(Y ~ ., data = train_scaled, kernel = "radial")

# Predictions on the validation set
svm_predictions <- predict(svm_model, validation_scaled)

# Converting probabilities to binary outcomes
threshold <- 0.5  # This is the default threshold, you can adjust it
binary_predictions <- ifelse(svm_predictions >= threshold, 1, 0)

# Calculate accuracy with binary outcomes
accuracy <- sum(binary_predictions == validation_scaled$Y) / nrow(validation_scaled)

# Output for submission
y_guesses <- predict(svm_model, validation_scaled[, -ncol(validation_scaled)])  # Predictions on test data
test_acc <- accuracy
team_name <- "YourTeamName"  # Replace with your team name
```

```{r}
# Save to RData file
save(y_guesses, test_acc, team_name, file = "predictions.RData")
```

