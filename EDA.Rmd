---
title: "EDA"
author: "Elizabeth Ho"
output: pdf_document
---

# Load in data, basic exploration of data
```{r}
# Load in the data
train <- read.csv("data/train_data.csv")
```

```{r}
full_model <- lm(Y ~ ., data = train)
summary(full_model)
```

```{r}
correlation_matrix <- cor(train[, -which(names(train) == "Y")])
correlation_matrix
```
## Observations from correlation matrix:

1. `Area` and `Perimeter`: Correlation coefficient is 0.989 (strong positive correlation).
2. `Area` and `MajorAxisLength`, `MinorAxisLength`, `ConvexArea`, `EquivDiameter`: All have correlation coefficients above 0.94 with `Area`.
3. `Perimeter` and `MajorAxisLength`, `MinorAxisLength`, `ConvexArea`, `EquivDiameter`: Similar to `Area`, these variables also show strong positive correlations with `Perimeter`.

Based on observations 1, 2, and 3, we could potentially only look at one of the variables (e.g., Area) since they all seem to be very correlated.

4. `AspectRation` and `Eccentricity`: Correlation coefficient is 0.981.
5. `Compactness`, `ShapeFactor1`, `ShapeFactor3` and `AspectRation`, `Eccentricity`: These groups have correlation coefficients very close to -1 with each other, indicating strong negative correlations.

Conclusions based on 4 and 5?

```{r}
library(corrplot)
# Circles correlation plot
corrplot(cor(train), method = "circle",
        tl.pos = "n", mar = c(2, 1, 3, 1)) 
```

```{r, fig.width=19, fig.height=9}
# Visualization of correlation matrix
corrplot.mixed(cor(train),
               lower = "number", 
               upper = "circle",
               tl.col = "black")
```
## NOTES
- Only use area, not perimeter, axis length, etc.

```{r}
# Check for multicollinearity
library(car)
vif_values <- vif(full_model)
rounded_vif <- round(vif_values, 4)
rounded_vif
```

Area, Perimeter, MajorAxisLength, MinorAxisLength, AspectRation, Eccentricity, ConvexArea, EquivDiameter, Compactness, ShapeFactor1, ShapeFactor2, and ShapeFactor3 all have very high VIF values, indicating extremely high multicollinearity.

Extent and ShapeFactor4 have VIF values around 1 and 220 respectively. It seems that Extent has little to no collinearity with other variables, while ShapeFactor4 is moderately correlated with others.

# Splitting data into training and validation

```{r}
# Splitting the data into training and validation sets
set.seed(1234)
index <- sample(1:nrow(train), round(0.8 * nrow(train)))
train_set <- train[index, ]
validation_set <- train[-index, ]
```

# LASSO
Do we need to standardize features?
```{r}
# NOTE: I did NOT standardize features; here is the code if we wanted to do so
# Standardize the features
standardized_train <- scale(train)
standardized_train <- as.data.frame(standardized_train)

# Fit the Lasso model on the standardized data
lasso_model_standardized <- cv.glmnet(as.matrix(standardized_train), train$Y, alpha = 1)
```

```{r, warning=FALSE, message=FALSE}
# Use Lasso?
library(glmnet)
x_train <- model.matrix(Y ~ . - 1, data = train_set) # -1 to exclude intercept
y_train <- train_set$Y
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)
plot(lasso_model)

# Predictions for the validation set
x_validation <- model.matrix(Y ~ . - 1, data = validation_set)
predictions <- predict(lasso_model, s = "lambda.min", newx = x_validation)
```

```{r}
# Extract coefficients of the model at lambda.min
lasso_coefficients <- coef(lasso_model, s = "lambda.min")

# Non-zero coefficients
print(round(lasso_coefficients[lasso_coefficients != 0], 6))
```

## Coefficients with variable names
```{r}
lasso_coefficients <- coef(lasso_model, s = "lambda.min")
variable_names <- colnames(x_train)
coef_w_names <- setNames(lasso_coefficients[-1], variable_names)
print(round(coef_w_names[coef_w_names != 0], 6))
```

Less significant variables:
- Area
- Perimeter
- Major Axis Length
- Convex Area
- EquivDiameter (?)

More significant variables:
- Shape factor 1
- Shape factor 2
- Shape factor 3
- Shape factor 4

Everything else TBD

```{r}
# Predict using the validation set
predictions <- predict(lasso_model, s = "lambda.min", newx = x_validation)

# MSE on the validation set
mse <- mean((validation_set$Y - predictions)^2)
mse
```

# Ridge

```{r}
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0)

# CV curve
plot(ridge_model)
```

```{r}
# Predictions for the validation set
ridge_predictions <- predict(ridge_model, s = "lambda.min", newx = x_validation)

# Extract coefficients of the Ridge model at lambda.min
ridge_coefficients <- coef(ridge_model, s = "lambda.min")

# Non-zero coefficients
print(round(ridge_coefficients, 6))
```

```{r}
# MSE for the validation set 
ridge_mse <- mean((validation_set$Y - ridge_predictions)^2)
ridge_mse
```

