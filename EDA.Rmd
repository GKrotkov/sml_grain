---
title: "EDA"
author: "Elizabeth Ho"
output: pdf_document
---

# Load in data
```{r}
# Load in the data
train <- read.csv("data/train_data.csv")
```

## Check the variable types
```{r}
str(train)
```

```{r}
full_model <- lm(Y ~ ., data = train)
summary(full_model)
```
# Visualizing data distributions to understand the characteristics of variables
(Adding in after observing multicollinearity)

## Area vs. perimeter scatterplot
```{r, warning=FALSE}
library(ggplot2)

ggplot(train, aes(x = Area, y = Perimeter)) +
  geom_point(alpha = 0.6, col = "cadetblue") +
  theme_minimal() +
  labs(title = "Perimeter vs Area",
       x = "Area",
       y = "Perimeter")
```
Looks very linearly related. But quick linear regression to check:
```{r}
perim_area <- lm(Perimeter ~ Area, data = train)
summary(perim_area)
```
Very highly correlated.

## MajorAxisLength vs. MinorAxisLength scatterplot
```{r}
ggplot(train, aes(x = MinorAxisLength, y = MajorAxisLength)) +
  geom_point(alpha = 0.6, col = "cadetblue") +
  theme_minimal() +
  labs(title = "MajorAxisLength vs. MinorAxisLength",
       x = "MinorAxisLength",
       y = "MajorAxisLength")
```
### Quick analysis of MajorAxisLength vs. MinorAxisLength Linear Model
To assess strength of correlation
```{r}
major_minor <- lm(MajorAxisLength ~ MinorAxisLength, data = train)
summary(major_minor)
```
Seems pretty strong

## PCA Biplot
```{r}
# library(ggplot2)
# library(GGally)
# 
# # Run PCA on the data
# pca_result <- prcomp(train[,sapply(train, is.numeric)], scale. = TRUE)
# 
# # Create biplot
# ggbiplot(pca_result,
#          obs.scale = 1,
#          var.scale = 1,
#          groups = train$Y,
#          ellipse = TRUE,
#          circle = TRUE) +
#   scale_color_discrete(name = '') +
#   theme_minimal() +
#   labs(title = "PCA Biplot with Observations Colored by 'Y'")
```


```{r}
correlation_matrix <- cor(train[, -which(names(train) == "Y")])
correlation_matrix
```
## Observations from correlation matrix:

1. `Area` and `Perimeter`: Correlation coefficient is 0.989 (strong positive correlation).
2. `Area` and `MajorAxisLength`, `MinorAxisLength`, `ConvexArea`, `EquivDiameter`: All have correlation coefficients above 0.94 with `Area`.
3. `Perimeter` and `MajorAxisLength`, `MinorAxisLength`, `ConvexArea`, `EquivDiameter`: Similar to `Area`, these variables also show strong positive correlations with `Perimeter`.

Based on observations 1, 2, and 3, we could potentially only look at one of the variables (e.g., Area) since they all seem to be very correlated.

4. `AspectRation` and `Eccentricity`: Correlation coefficient is 0.981.
5. `Compactness`, `ShapeFactor1`, `ShapeFactor3` and `AspectRation`, `Eccentricity`: These groups have correlation coefficients very close to -1 with each other, indicating strong negative correlations.

Conclusions based on 4 and 5?

```{r}
library(corrplot)
# Circles correlation plot
corrplot(cor(train), method = "circle",
        tl.pos = "n", mar = c(2, 1, 3, 1)) 
```

```{r, fig.width=19, fig.height=9}
# Visualization of correlation matrix
corrplot.mixed(cor(train),
               lower = "number", 
               upper = "circle",
               tl.col = "black")
```
## NOTES
- Only use area, not perimeter, axis length, etc.

```{r}
# Check for multicollinearity
library(car)
vif_values <- vif(full_model)
rounded_vif <- round(vif_values, 4)
rounded_vif
```

Area, Perimeter, MajorAxisLength, MinorAxisLength, AspectRation, Eccentricity, ConvexArea, EquivDiameter, Compactness, ShapeFactor1, ShapeFactor2, and ShapeFactor3 all have very high VIF values, indicating extremely high multicollinearity.

Extent and ShapeFactor4 have VIF values around 1 and 220 respectively. It seems that Extent has little to no collinearity with other variables, while ShapeFactor4 is moderately correlated with others.

# Outliers - Detection and Handling
```{r}

```

# Missing data
```{r}

```

# Feature Engineering and Selection:
Based on your EDA, decide which variables to keep, drop, or transform.
Create new features that might be useful for your model, based on your understanding of the dataset and the relationships among variables.
Perform any necessary transformations (e.g., scaling, normalization) that might help your modeling process.

   
# MSE values really small - checking to see if anything's wrong
```{r}
predicted_classes <- ifelse(predictions > 0.5, 1, 0) 

library(caret)
conf_matrix <- confusionMatrix(factor(predicted_classes), factor(validation_set$Y))
print(conf_matrix)
```

